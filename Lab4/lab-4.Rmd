---
title: "Lab 4: Classification"
author: "Dean Young - Vikram Chan-Herur"
output: 
  html_document: 
    highlight: pygments
    theme: spacelab
---

```{r, include=FALSE}
library(MASS)
library(dplyr)
library(boot)
library(ggplot2)

war <- read.csv("http://www.stat.cmu.edu/~cshalizi/uADA/15/hw/06/ch.csv")
war <- war[,-1] # drop the first col of indices
war <- na.omit(war)
```

# Question 1

**Fit a logistic regression model for the start of civil war on all other variables except country and year (yes, this makes some questionable assumptions about independent observations); include a quadratic term for exports. Report the coefficients and their standard errors, together with R's p-values. Which ones does R say are significant at the 5% level?**

```{r}
m1 <- war %>% glm(data=.,start ~ schooling + growth + peace 
    + concentration + lnpop + fractionalization + dominance + exports + I(exports^2), family="binomial")
summary(m1)
```
See table above for coefficients, SEs, and p-values.
All but dominance are significant at the 5% level, although dominance is close: p = 0.058.

# Question 2
Interpretation: All parts of this question refer to the logistic regression model you just fit.

**What is the model’s predicted probability for a civil war in India in the period beginning 1975? What probability would it predict for a country just like India in 1975, except that its male secondary school enrollment rate was 30 points higher? What probability would it predict for a country just like India in 1975, except that the ratio of commodity exports to GDP was 0.1 higher?**

```{r}
p_IN_75 <- war %>% filter(country=="India",year==1975) %>% predict(newdata=.,m1) %>% inv.logit() 
p_IN_ss <- war %>% filter(country=="India",year==1975) %>% mutate(schooling=schooling+30) %>% 
  predict(newdata=.,m1) %>% inv.logit()
p_IN_gdp <- war %>% filter(country=="India", year==1975) %>% mutate(exports=exports+0.1) %>%
  predict(newdata=.,m1) %>% inv.logit()
```
The predicted probability for a civil war in India beginning in 1975 is `r p_IN_75`. For a country like India but with higher enrolment, we'd predict a probability of `r p_IN_ss`. With higher exports, we'd predict `r p_IN_gdp`.

**What is the model’s predicted probability for a civil war in Nigeria in the period beginning 1965? What probability would it predict for a country just like Nigeria in 1965, except that its male secondary school enrollment rate was 30 points higher? What probability would it predict for a country just like Nigeria in 1965, except that the ratio of commodity exports to GDP was 0.1 higher?**
```{r}
p_NG_65 <- war %>% filter(country=="Nigeria",year==1965) %>% predict(newdata=.,m1) %>% inv.logit() 
p_NG_ss <- war %>% filter(country=="Nigeria",year==1965) %>% mutate(schooling=schooling+30) %>% 
  predict(newdata=.,m1) %>% inv.logit()
p_NG_gdp <- war %>% filter(country=="Nigeria", year==1965) %>% mutate(exports=exports+0.1) %>%
  predict(newdata=.,m1) %>% inv.logit()
```
The predicted probability for a civil war in Nigeria beginning in 1965 is `r p_NG_65`. For a country like India but with higher enrolment, we'd predict a probability of `r p_NG_ss`. With higher exports, we'd predict `r p_NG_gdp`.

**In the parts above, you changed the same predictor variables by the same amounts. If you did your calculations properly, the changes in predicted probabilities are not equal. Explain why not. (The reasons may or may not be the same for the two variables.)**

The different changes are due to the logistic regression: because the function does not have a constant slope, the response to change depends on where in the data the observations are. 

# Question 3
**Build a 2 × 2 confusion matrix (a.k.a. “classification table” or “contigency table”) which counts: the number of outbreaks of civil war correctly predicted by the logistic regression; the number of civil wars not predicted by the model; the number of false predictions of civil wars; and the number of correctly predicted absences of civil wars. (Note that some entries in the table may be zero.)**

```{r}
my_log_pred <- ifelse(m1$fit < 0.5, "No", "Yes")
war$start <- ifelse(war$start==0,"No","Yes")
conf_log <- table(my_log_pred, war$start)
conf_log
```

**What fraction of the logistic regression’s predictions are incorrect, i.e. what is the misclassification rate? (Note that this is if anything too kind to the model, since it’s looking at predictions to the same training data set).**

The misclassification rate is `r (1/nrow(war)) * (conf_log[2, 1] + conf_log[1, 2])`.

**Consider a foolish (?) pundit who always predicts “no war”. What fraction of the pundit’s predictions are correct on the whole data set? What fraction are correct on data points where the logistic regression model also makes a prediction?**

The foolish pundit would predict `r 46/nrow(war)`.

# Question 4
**Comparison: Since this is a classification problem with only two classes, we can compare Logistic Regression right along side Discriminant Analysis.**

**Fit an LDA model using the same predictors that you used for your logistic regression model. What is the training misclassification rate?**

```{r}
war$start <- as.factor(war$start)
m2 <- lda(start ~ schooling + growth + peace 
    + concentration + lnpop + fractionalization + dominance + exports + I(exports^2), data=war)

conf_lda <- table(predict(m2)$class, war$start)
conf_lda
```
The LDA has a misclassification rate of `r (1/nrow(war)) * (conf_lda[2, 1] + conf_lda[1, 2])`.

**Fit a QDA model using the very same predictors. What is the training misclassification rate?**

```{r}
m3 <- qda(start ~ schooling + growth + peace 
    + concentration + lnpop + fractionalization + dominance + exports + I(exports^2), data=war)
conf_qda <- table(predict(m3)$class, war$start)
conf_qda
```
The QDA has a misclassification rate of `r (1/nrow(war)) * (conf_qda[2, 1] + conf_qda[1, 2])`.

**How does the prediction accuracy of the three models compare? Why do you think this is?**

None of the three models does better than the naïve pundit. Of the three models, the QDA model has a higher misclassification rate but is more likely to predict war correctly (as a result of its less conservative estimates). This may be because the event is so rare and we don't have that many observations so it's hard to make predictions. Additionally, we're assuming that all our observations are independent but in fact they are highly correlated (countries over time).

#Challenge problem: 
**Using the code available in the slides titled qda.Rmd, construct an ROC curve for your logistic regression model. For an extra challenge, plot the ROC curves of all three models on the same plot.**

```{r, echo=FALSE, cache=TRUE}
# Function that generates a data-frame for data for an ROC curve - logistic regression only
data_log_ROC <- function(model, nthresh = 1000) {
  k <- seq(0, 1, length.out = nthresh)
  TPR <- rep(NA, nthresh)
  FPR <- rep(NA, nthresh)
  for(i in 1:nthresh) {
    pred <- as.factor(ifelse(model$fit < k[i], "No", "Yes"))
    if (length(levels(pred)) == 1 & levels(pred)[1] == "Yes") {levels(pred) <- c("Yes", "No")}
    if (length(levels(pred)) == 1 & levels(pred)[1] == "No") {levels(pred) <- c("No", "Yes")}
    conf <- table(pred, war$start)
    TPR[i] <- conf["Yes", "Yes"]/ sum(conf[, "Yes"])
    FPR[i] <- conf["Yes", "No"]/ sum(conf[, "No"])
  }
  df <- data.frame(TPR, FPR) %>% mutate(Type="Log")
  
  return(df)
 
}

# Function that generates a data-frame for data for an ROC curve - lda or qda regression only
data_da_ROC <- function(model, nthresh = 1000) {
  k <- seq(0, 1, length.out = nthresh)
  TPR <- rep(NA, nthresh)
  FPR <- rep(NA, nthresh)
  for(i in 1:nthresh) {
    p <- k[i]
    pred <- predict(model,prior=c(p,1-p))[["class"]]
    if (length(levels(pred)) == 1 & levels(pred)[1] == "Yes") {levels(pred) <- c("Yes", "No")}
    if (length(levels(pred)) == 1 & levels(pred)[1] == "No") {levels(pred) <- c("No", "Yes")}
    conf <- table(pred, war$start)
    TPR[i] <- conf["Yes", "Yes"]/ sum(conf[, "Yes"])
    FPR[i] <- conf["Yes", "No"]/ sum(conf[, "No"])
  }
  
  df <- data.frame(TPR, FPR) %>% mutate(Type=ifelse(model$call[1]=="qda()","QDA","LDA"))
  
  return(df)
 
}

roc_data <- bind_rows(data_log_ROC(m1),data_da_ROC(m2)) %>% bind_rows(data_da_ROC(m3))

ggplot(roc_data, aes(x = FPR, y = TPR)) +
    geom_smooth(aes(col=Type), se=FALSE) +
    xlab("False Positive Rate") +
    ylab("True Positive Rate") +
    ggtitle("ROC Curves")

```

In terms of ROC, it appears that QDA outperformed the LDA and Logistic regression models since it has the greatest area under the curve.