---
title: "Lab 6: Trees"
author: Dean Young - Hans Trautlein
output: 
  html_document: 
    highlight: pygments
    theme: spacelab
---

```{r, include=FALSE}
library(dplyr)
library(ggplot2)
library(tree)
library(randomForest)
```


### When a guest arrives they will count how many sides it has on

In class, we estimated by eye the first split in a classification tree for the following shapely data set. Now let's check to see if our graphical intuition agrees with that of the full classification tree algorithm.

```{r, echo = FALSE, fig.height=4.5, fig.width = 4.8, fig.align='center'}
set.seed(75)
n <- 16
x1 <- runif(n)
x2 <- runif(n)
group <- as.factor(sample(1:3, n, replace = TRUE))
levels(group) <- c("circle", "triangle", "square")
df <- data.frame(x1, x2, group)
df[1, 2] <- .765 # tweaks to make a more interesting configuration
df[9, 1] <- .741
df <- df[-7, ]

ggplot(df, aes(x = x1, y = x2, col = group, shape = group)) +
  geom_point(size = 4) +
  scale_x_continuous(expand = c(0, 0) , limits = c(0, 1)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 1)) +
  scale_color_discrete(guide = FALSE) +
  scale_shape_discrete(guide = FALSE) +
  theme_bw()

```

#### 1. Growing the full classification tree

Use the `trees` package in R to fit a full unpruned tree to this data set, making splits based on the *Gini index*. You can find the code to do this in the slides from week 8 or in the lab at the end of Chapter 8 in the book. Please plot the resulting tree.

```{r}
t1 <- tree(group ~ ., data=df, split = "gini")
plot(t1)
text(t1, pretty = 0)
```


a. The two most common splits that we saw in class were a horizontal split around $X_2 \approx 0.50$ and a vertical split around $X_1 \approx 0.30$. Was either of these the first split decided upon by your classification tree?

**Neither of these splits were decided upon. In fact, x1 was not even used oddly enough.**

b. What is the benefit of the second split in the tree?

**There is no benefit, really. The second split has the same prediction for both leaves, square. This likely occurred because splitting there resulted in decreased Gini Index due to decreases in the proportion of the other classes (circle and triangle) to achieve greater nodal purity. However, square still remained the plurality in both groups.**

c. Which class would this model predict for the new observation with $X_1 = 0.21, X_2 = 0.56$?

**Square**

#### 2. An alternate metric

Now refit the tree based on the *deviance* as the splitting criterion (you set this as an argument to the `tree()` function). The deviance is defined for the classification setting as:

$$ -2 \sum_m \sum_k n_{mk} \log \hat{p}_{mk}$$

Plot the resulting tree. Why does this tree differ from the tree fit based on the Gini Index?

```{r}
t2 <- tree(group ~ ., data=df, split = "deviance")
plot(t2)
text(t2, pretty = 0)
```

**The tree differs most likely due to the appearance of $n_{mk}$ in the splitting criterion. Now you are weighing the nodal purity of each region with the absolute number of observations in that region. This puts a foot on the brakes of the algorithm continually making splits just for the sake of getting more nodal purity, since more splits means the region gets split into two regions with less observations each. It may not be worth splitting a region even if nodal purity increases because they would each get less weight in reducing deviance since $n_{mk}$ decreases.**

**This is likely why when deviance is used as the splitting criterion, it didn't make an additional second split unlike when Gini is used.**

* * *

### Crime and Communities, revisited

In Lab 3, you fit a regression model to a training data set that predicted the crime rate in a community as a function of properties of that community.

#### 3. Growing a pruned regression tree
Fit a regression tree to the *training* data using the default splitting criteria (here, the deviance is essentially the RSS). Next, perform cost-complexity pruning and generate a plot showing the relationship between tree size and deviance to demonstrate the size of the best tree. Finally, construct the tree diagram for this best tree.

```{r fig.height=10}
set.seed(76)

# Import data
crime_train <- read.csv('crime-train.csv') %>% tbl_df()
crime_train[crime_train == '?'] <- NA
crime_test <- read.csv('crime-test.csv') %>% tbl_df() 
crime_test[crime_test == '?'] <- NA

# Subset the data (getting rid of columns that aren't predictors). Col 127 is our response
crime_train_subset <- crime_train[c(5:100, 127)] 
crime_test_subset <- crime_test[c(5:100, 127)] 

# Fit tree with training data
t3 <- tree(ViolentCrimesPerPop ~ ., data=crime_train_subset)
plot(t3)
text(t3, pretty=0)

# CV to select best size of tree
cv.tree.results <- cv.tree(t3)
cv.tree.results

# Relationship between size and deviance
plot(cv.tree.results$size, cv.tree.results$dev, type="b")

# Pruned tree with size = 13
prune.crime <- prune.tree(t3,best=13)
plot(prune.crime)
text(prune.crime, pretty = 0)

# The two trees plotted one above the other.
# Notice differences in on left side of both trees in the node beginning with
# % speaking english well. New node based on total % divorced
par(mfrow=c(2,1))
plot(t3)
text(t3, pretty=0)
plot(prune.crime)
text(prune.crime, pretty = 0)
```

#### 4. Comparing predictive performance
Use this tree to compute the MSE for the *test* data set. How does it compare to the test MSE for your regression model?

```{r}
yhat <- predict(prune.crime, newdata=crime_test_subset)
MSE <- mean((yhat - crime_test_subset$ViolentCrimesPerPop)^2)
```


**The test MSE computed for the test dataset is *0.02618434*.**
**The test MSE computed for the test dataset with Deans' groups' (H) regression was *0.01817328*.**
**The test MSE computed for the test dataset with Hans' groups' (B) regression was *0.01776706*.**

**It appears that the pruned tree does not outperform a regression model in this case.**


#### 5. Growing a random forest
We now apply methods to decrease the variance of our estimates. Fit a `randomForest()` model that performs only bagging and no actual random forests (recall that bagging is the special case of random forests with $m = p$). Next, fit a second random forest model that uses $m = p/3$. Compute their test MSEs. Is this an improvement over the vanilla pruned regression tree? Does it beat your regression model?

```{r, cache=TRUE}
# Bagging (random forest with m = p = 96)
bag.crime <- randomForest(ViolentCrimesPerPop ~ ., data=crime_train_subset, mtry=96, importance=TRUE)

# Random forest with m = p/3
rf.crime <- randomForest(ViolentCrimesPerPop ~ ., data=crime_train_subset, importance=TRUE)

bag.yhat <- predict(bag.crime, newdata=crime_test_subset)
bag.MSE <- mean((bag.yhat - crime_test_subset$ViolentCrimesPerPop)^2)

rf.yhat <- predict(rf.crime, newdata=crime_test_subset)
rf.MSE <- mean((rf.yhat - crime_test_subset$ViolentCrimesPerPop)^2)
```

**The bagged tree test MSE is `r bag.MSE` and the random forest tree test MSE is `r rf.MSE`.**


#### 6. Variance importance
One thing we lose by using these computational techniques to limit the variance is the clearly interpretable tree diagram. We can still salvage some interpretability by considering `importance()`. Please construct a Variable Importance Plot (`varImpPlot()`). Are these results similar/different from your interpretation of your regression coefficients in Lab 3?

```{r}
importance(rf.crime)
varImpPlot(rf.crime)
```

**When we discussed Lab 3 race was often brought up as the most important predictor in the data set. Here that is recognized by the use of `racePctWhite` which is in the top five most important predictors. However some variables while included in our labs, were not discussed in the classroom much. For example, the family units were not properly accounted for as `PctKids2Par`, `PctIlleg`, `NumIlleg`, and `TotalPctDiv` are all highly important variables (there are even more family related variables that are important that aren't listed here because they exist in the importance plot above). So while some labs did include the family predictors their importance wasn't acknowledged and was downplayed compared to the racial predictors. These racial predictors are still explanatory, but they aren't as powerful as the family predictors.**


