---
title: "Lab 7: Boosting"
author: "Dean Young"
output: 
  html_document: 
    highlight: pygments
    theme: spacelab
---

```{r, include=FALSE}
library(dplyr)
library(ggplot2)
library(gbm)
library(knitr)

```


<img src="http://andrewpbray.github.io/math-243/assets/figs/letters.png" align = "middle"> 

### Ransom notes keep falling

One of the most useful applications to come out of classification models has been character (i.e. letter) recognition. In this lab, we build our own character recognition system using boosted trees.

#### The data
Our data set consists of a catalog of 20,000 images of letters. Initially, the images were 45 x 45 pixels, each of which was characterized as either "on" or "off" (black or white). In order to extract more meaningful predictors from the data, resesearchers [^1] went through and performed *feature extraction*, collapsing those 20255 dimensions into 16, each of which is a summary statistic calculated on the image. They are as follows:

1. The horizontal position, counting pixels from the left edge of the image, of the center of the smallest rectangular box that can be drawn with all "on" pixels inside the box.
2. The vertical position, counting pixels from the bottom, of the above box.
3. The width, in pixels, of the box.
4. The height, in pixels, of the box.
5. The total number of "on" pixels in the character image.
6. The mean horizontal position of all "on" pixels relative to the center of the box and divided by the width of the box. This feature has a negative value if the image is "left- heavy" as would be the case for the letter L.
7. The mean vertical position of all "on" pixels relative to the center of the box and divided by the height of the box.
8. The mean squared value of the horizontal pixel distances as measured in 6 above. This attribute will have a higher value for images whose pixels are more widely separated in the horizontal direction as would be the case for the letters W or M.
9. The mean squared value of the vertical pixel distances as measured in 7 above. 
10. The mean product of the horizontal and vertical distances for each "on" pixel as measured in 6 and 7 above. This attribute has a positive value for diagonal lines that run from bottom left to top right and a negative value for diagonal lines from top left to bottom right.
11. The mean value of the squared horizontal distance times the vertical distance for each "on" pixel. This measures the correlation of the horizontal variance with the vertical position.
12. The mean value of the squared vertical distance times the horizontal distance for each "on" pixel. This measures the correlation of the vertical variance with the horizontal position.
13. The mean number of edges (an "on" pixel immediately to the right of either an "off" pixel or the image boundary) encountered when making systematic scans from left to right at all vertical positions within the box. This measure distinguishes between letters like "W" or "M" and letters like 'T' or "L."
14. The sum of the vertical positions of edges encountered as measured in 13 above. This feature will give a higher value if there are more edges at the top of the box, as in the letter "Y."
15. The mean number of edges (an "on" pixel immediately above either an "off" pixel or the image boundary) encountered when making systematic scans of the image from bottom to top over all horizontal positions within the box.
16. The sum of horizontal positions of edges encountered as measured in 15 above.

In addition, each row/image was labeled with the letter that it corresponds to.

You will want to build your model on a training data set and evaluate its performance on a separate test data set. Please use the following indices to subset out the training data set, leaving the remaining as test.

```{r}
# read.csv with no header
lettersdf <- read.csv("letters.csv", header=FALSE)

# 1st column is letter
names(lettersdf)[1] <- "letter"

# Properly label remaining variables with V1...V16
for(i in 2:length(names(lettersdf))){
  names(lettersdf)[i] <- paste("V",i-1, sep='')
}

# Create training data
set.seed(1)
train <- sample(1:nrow(lettersdf), nrow(lettersdf) * .75)
letters_train <- slice(lettersdf, train)

# Test data
letters_test <- slice(lettersdf, -train)

#write.csv(letters_train,"letters_train.csv",row.names=FALSE)
#write.csv(letters_test,"letters_test.csv",row.names=FALSE)
```


#### Building a boosted tree

Contruct a boosted tree to predict the class of the training images (the letters) based on its 16 features. This can be done with the `gbm()` function in the library of the same name. Look to the end of chapter 8 for an example of the implementation. Note that we'll be performing a boosted *classification* tree. It's very similar to the boosted regression tree except the method of calculating a residual is adapted to the classification setting. Please use as your model parameters $B = 50$, $\lambda = 0.1$, and $d = 1$. Note that this is computationally intensive, so it may take a minute to run. Which variable is found to be the most important?

```{r, cache=TRUE}

set.seed(76)
b1 <- gbm(letter ~ ., distribution = "multinomial", data=letters_train, n.trees=50, interaction.depth=1, shrinkage=0.1)

summary(b1)

```

**Variable 12 is by far the most important. Honorable mention goes to Variable 13,10, and 11.**

#### Assessing predictions

Now use this boosted model to predict the classes of the images in the training data set. Use the same number of trees and be sure to add the argument `type = "response"`. The output of this will be a 5000 X 26 X 1 array: for each image you'll have a predicted probability that it is from each of the 26 classes. To extract the vector of length 5000 of each final predicted class, you can use the following function.

**I think you mean testing data set?**

```{r}
# Predict - gives likelihoods of each letter for test set
yhat <- predict(b1,newdata=letters_test, n.trees=50)

# Extract highest likelihood letter
predicted <- LETTERS[apply(yhat, 1, which.max)]
```

Where `yhat` is the output of the `predict()` function.

a. Build a cross-tabulation of the predicted and actual letters (a 26 X 26 confusion matrix).
b. What is your misclassification rate? (the function `diag()` might be helpful)
c. What letter was most difficult to predict?
d. Are there any letter pairs that are particularly difficult to distinguish?

```{r, kable}
# a.
conf_mat <- table(predicted, letters_test$letter)
pretty_conf_mat <- kable(conf_mat)
pretty_conf_mat
```

```{r}
# b.
missclass <- (sum(conf_mat)-sum(diag(conf_mat)))/sum(conf_mat)

# c.
# Create a dataframe of predictions and actual letteres
diff <- data.frame(as.vector(predicted),letters_test$letter)
colnames(diff) <- c("predicted","letter")

# Group by actual letters and summarise the missclassification rate
diff %>% group_by(letter) %>% summarise(mc_rate = sum(predicted != letter)/n()) %>% 
  arrange(desc(mc_rate))

# d.


# Finds the sum of c[i,j] + c[j,i] (for i != j) 
# ie. the total missclassification for a letter pair
pair_missclass <- function(conf_mat){
  
  pair_miss <- data.frame()
  
  for(i in 2:nrow(conf_mat)){
    for(j in 1:(i-1)){
      num <- conf_mat[i,j] + conf_mat[j,i] # Total missclassified
      denom <- sum(conf_mat[, i]) + sum(conf_mat[, i]) # Total appearance of either letter in test data
      
      mc_rate <- num/denom
        
      a <- data.frame(paste(colnames(conf_mat)[i],",",rownames(conf_mat)[j],sep=""), mc_rate)
  
      pair_miss <- bind_rows(pair_miss,a)
  
    }
  
  }
  
  colnames(pair_miss) <- c("Pair","Missclassed_Rate")
  return(pair_miss)
  
}

pair_miss <- pair_missclass(conf_mat)
colnames(pair_miss) <- c("Pair","Missclassed_Rate")
pair_miss %>% arrange(desc(Missclassed_Rate))

```

**b. Test missclassification rate was `r missclass`.**

**c. The letter E was by far the most difficult to predict.**

**d. From `pair_miss` it appears that the pair of D and B had to highest total missclassification rate at 12%. This seems reasonable as the two letters look similar. E actually appears quite often in the highest missclassified pairs list, which makes sense considering our answer to part c.**


#### Slow the learning

Build a second boosted tree model that uses even *slower* learners, that is, decrease $\lambda$ and increase $B$ somewhat to compensate (the slower the learner, the more of them we need). Pick the parameters of your choosing for this, but be wary of trying to fit a model with too high a $B$. You don't want to wait an hour for your model to fit.

a. How does the misclassification rate compare to the rate from you original model?
b. Are there any letter pairs that became particularly easier/more difficult to distinguish?

```{r, cache=TRUE}
# a.
set.seed(76)
b2 <- gbm(letter ~ ., distribution = "multinomial", data=letters_train, n.trees=100, interaction.depth=1, shrinkage=0.01)
summary(b2)

yhat2 <- predict(b2,newdata=letters_test, n.trees=100)
predicted2 <- LETTERS[apply(yhat2, 1, which.max)]
conf_mat2 <- table(predicted2, letters_test$letter)
missclass2 <- (sum(conf_mat2)-sum(diag(conf_mat2)))/sum(conf_mat2)

# b.
pair_miss2 <- pair_missclass(conf_mat2)
pair_miss2 %>% arrange(desc(Missclassed_Rate))


```

**a. The new missclassification rate is `r missclass2` which is actually significantly worse than the first boosted model.**

**b. Y and V are now the most difficult to distinguish pair. This pair did not appear the top 10 most difficult pairs in the previous model. D and B the previous most difficult to distinguish pair is now the 5th most difficult, but is still more difficult in absolute (14% vs 12% total missclassification).**

```{r, cache=TRUE}
# a.
set.seed(76)
b3 <- gbm(letter ~ ., distribution = "multinomial", data=letters_train, n.trees=100, interaction.depth=1, shrinkage=0.05)
summary(b3)

yhat3 <- predict(b3,newdata=letters_test, n.trees=100)
predicted3 <- LETTERS[apply(yhat3, 1, which.max)]
conf_mat3 <- table(predicted3, letters_test$letter)
missclass3 <- (sum(conf_mat3)-sum(diag(conf_mat3)))/sum(conf_mat3)

# b.
pair_miss3 <- pair_missclass(conf_mat3)
pair_miss3 %>% arrange(desc(Missclassed_Rate))


```

* * *

### Communities and Crime

Return to the Communities and Crime data set. In the last lab you added bagged trees and random forests to your model portfolio in trying to predict the crime level. Constructed model based on a boosted tree with parameters of your choosing. How does the test MSE compare to your existing models?

```{r, cache=TRUE}
set.seed(76)

# Import data
crime_train <- read.csv('crime-train.csv') %>% tbl_df()
crime_train[crime_train == '?'] <- NA
crime_test <- read.csv('crime-test.csv') %>% tbl_df() 
crime_test[crime_test == '?'] <- NA

# Subset the data (getting rid of columns that aren't predictors). Col 127 is our response
crime_train <- crime_train[c(5:100, 127)] 
crime_test <- crime_test[c(5:100, 127)] 

# Boosted model for crime
crime_boost <- gbm(ViolentCrimesPerPop ~ ., data=crime_train, distribution="gaussian", 
                   n.trees=50, interaction.depth=1, shrinkage=0.1)
summary(crime_boost)

# Test MSE
yhat_crime <- predict(crime_boost, newdata=crime_test, n.trees=50)
MSE <- mean((yhat_crime - crime_test$ViolentCrimesPerPop)^2)

```

**The variables that were found to be important were very similar to the ones found by the bagging and random forest models (family related variables and race).**

**The previous test MSEs of the bagged model (0.0169559) and random forest (0.0171044) were very similar to this boosted model test MSE (`r MSE`).**

[^1]: P. W. Frey and D. J. Slate. "Letter Recognition Using Holland-style Adaptive Classifiers". (Machine Learning Vol 6 #2 March 91) 
